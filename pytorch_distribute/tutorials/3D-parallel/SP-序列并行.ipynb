{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36b4a157-3b79-4712-bfdc-86357e4ec936",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T02:29:03.933211Z",
     "iopub.status.busy": "2025-04-20T02:29:03.932608Z",
     "iopub.status.idle": "2025-04-20T02:29:03.944443Z",
     "shell.execute_reply": "2025-04-20T02:29:03.941898Z",
     "shell.execute_reply.started": "2025-04-20T02:29:03.933143Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6975662b-8486-4557-a621-bceb762d986b",
   "metadata": {},
   "source": [
    "- 优化长序列（long sequence，1M context window）的问题；\n",
    "    - DP, TP, PP & SP\n",
    "    - 长序列拆分到不同的设备上计算，每个设备处理 sub seq；\n",
    "- https://arxiv.org/pdf/2105.13120\n",
    "    - Sequence Parallelism: Long Sequence Training from System Perspective\n",
    "- https://arxiv.org/pdf/2309.14509\n",
    "    - DeepSpeed Ulysses: System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e917bf88-6a96-4b06-8a73-1001549d6ed8",
   "metadata": {},
   "source": [
    "### Ring Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d711dbb-91fe-4fb8-b192-820a248b3d52",
   "metadata": {},
   "source": [
    "- Ring-AllReduce：通信换内存\n",
    "    - 序列 split/shard 到多张卡上，即每张卡只保存一个 sub seq；\n",
    "    - (Ring)QK & (Ring)AV\n",
    "        - 每个 device sub seq 的 Query 需要跟其他 devices 上的所有的 Key 做计算；\n",
    "$$\\text{Attention}(Q, K, V) = \\underbrace{ \\text{softmax}\\left( \\frac{QK^{\\top}}{\\sqrt{d_k}} \\right) }_{\\mathbf{A}} V$$\n",
    "\n",
    "- N 个 devices，N-1 次 iter，每个 device 都有完整的 QK^T 的结果\n",
    "\n",
    "$$\n",
    "\\underset{\\substack{\\uparrow \\\\ (b, n, d_v)}}{\\text{Attention}(Q, K, V)} = \\underbrace{\\text{softmax} \\left( \\frac{\\overbrace{\\underset{\\substack{\\uparrow \\\\ (b, n, d_k)}}{Q} \\cdot \\underset{\\substack{\\uparrow \\\\ (b, d_k, n)}}{K^T}}^{\\text{Scores Dim: }(b, n, n)}}{\\underset{\\substack{\\uparrow \\\\ \\text{scalar}}}{\\sqrt{d_k}}} \\right)}_{\\text{Weights Dim: }(b, n, n)} \\cdot \\underset{\\substack{\\uparrow \\\\ (b, n, d_v)}}{V}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e8c1716-c09f-408c-810d-07e89fed3358",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T02:29:33.753100Z",
     "iopub.status.busy": "2025-04-20T02:29:33.752417Z",
     "iopub.status.idle": "2025-04-20T02:29:33.767156Z",
     "shell.execute_reply": "2025-04-20T02:29:33.764804Z",
     "shell.execute_reply.started": "2025-04-20T02:29:33.753036Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./imgs/ring-attn.png\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image(url='./imgs/ring-attn.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee76262-acc8-43ab-92f3-844ebfbbe940",
   "metadata": {},
   "source": [
    "### DeepSpeed Ulysses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de50011d-5ff9-4d72-8ff7-c7c62f67e13a",
   "metadata": {},
   "source": [
    "- Ulysses：尤利西斯（a very long novel)；\n",
    "- all-to-all communication collective\n",
    "    - DeepSpeed-Ulysses partitions individual samples along the sequence dimension among participating GPUs.\n",
    "    - Then right before the attention computation, it employs all-to-all communication collective on the **partitioned queries, keys and values** such that each GPU receives the full sequence but only for a **non-overlapping subset of the attention heads**. This allows the participating GPUs to compute attention for different attention heads in parallel.\n",
    "        - **gather_seq_scatter_heads**\n",
    "    - Finally, DeepSpeed-Ulysses employs another all-to-all to **gather the results along the attention heads** while re-partitioning along the sequence dimension.\n",
    "        - **gather_heads_scatter_seq**\n",
    "- 将输入序列 X (长度 N) 沿序列维度切分为 SP 块，每个 GPU 分配到 N/SP 长度的子序列。\n",
    "    - 对于非注意力层 (如 MLP)，计算是完全局部的，每个 GPU 处理自己的子序列即可。\n",
    "        - token 之间独立，token-level projection\n",
    "        - Ulysses SP的核心复杂性在于Attention层。为了让每个token在计算注意力时能够考虑到全局序列信息（或者说，让每个head在计算时能看到完整的序列，即使这个head只在当前rank计算），Attention模块前后需要进行两次精密的all-to-all数据重排。MLP层则没有这样的需求，数据在进入MLP时已经是按序列分片好的，可以直接进行本地计算。\n",
    "    - 对于注意力层:\n",
    "        - 步骤 1 (计算 Q, K, V): 每个 GPU 基于其本地子序列计算出本地的 Q_local, K_local, V_local (维度约为 N/SP x d，d 是隐藏维度)。\n",
    "        - 步骤 2 (全局 K, V 收集 - 关键): 使用 **All-to-All** 通信操作（All-Gather??）。每个 GPU 将自己的 K_local, V_local 发送给所有其他 GPU，并接收来自所有其他 GPU 的 K, V 块。执行后，**每个 GPU 拥有完整的全局 K 和 V 矩阵 (维度 N x d)**，但仍然只拥有本地的 Q_local (维度 N/SP x d)。\n",
    "            - https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html\n",
    "        - 步骤 3 (本地注意力计算): 每个 GPU 使用其 Q_local 和完整的全局 K, V 计算其负责的那部分注意力输出 O_local (维度 N/SP x d)。计算公式为 Attention(Q_local, K_global, V_global)。这一步的计算量是 (N/SP) * N * d，内存瓶颈在于存储临时的注意力分数矩阵，大小约为 **(N/SP) * N**。相比原始的 **N*N**，内存显著降低。\n",
    "        - 步骤 4 (可选的输出重组): 如果后续层需要按序列拼接的完整输出，可能需要另一次通信（如 All-Gather 或另一次 All-to-All 的变种）来组合 O_local。但在 DeepSpeed 实现中，通常保持分布式状态，直接输入到下一个同样按序列并行的层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529320ea-aa0b-4d27-a9cf-f3a28300ff90",
   "metadata": {},
   "source": [
    "### verl sp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a54d6a3-04c7-4077-b304-2426fba807bc",
   "metadata": {},
   "source": [
    "- `torchrun --nproc_per_node=2 -m pytest tests/model/test_transformers_ulysses.py -svv`\n",
    "    - dp_size = world_size // sp_size\n",
    "- monkey_patch\n",
    "    - `_flash_attention_forward` => `_ulysses_flash_attention_forward`\n",
    "    - 假设序列并行数 `ulysses_sp_size = N`。每个SP rank最初拥有 `(batch_size, seq_len / N, num_heads, head_dim)` 形状的 Q, K, V 张量。\n",
    "        - gather_seq_scatter_heads\n",
    "            - `[bsz, seq/n, h, ...] -> [bsz, seq, h/n, ...]` （for Q/K/V）\n",
    "                - 得到完整的序列，部分的头；\n",
    "        - flash-attn => `[bsz, seq, h/n, ...]`\n",
    "        - gather_heads_scatter_seq\n",
    "            - `[bsz, seq, h/n, ...] -> [bsz, seq/n, h, ...]`\n",
    "                - 得到部分的序列，完整的头；\n",
    "- 数据并行（fsdp）与 sp\n",
    "    - fsdp：优化的是模型参数所占显存，sp：优化的是激活所占显存\n",
    "    - fsdp: all-gather, reduce-scatter\n",
    "    - sp: all-to-all\n",
    "\n",
    "```\n",
    "      SP=4 (列) -->\n",
    "DP=2  GPU(0,0) GPU(0,1) GPU(0,2) GPU(0,3)  <-- DP Group 0 (Row 0)\n",
    "(行)  GPU(1,0) GPU(1,1) GPU(1,2) GPU(1,3)  <-- DP Group 1 (Row 1)\n",
    " |\n",
    " V\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a09e825f-59f7-4dc1-b397-38233f402789",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T02:48:09.460418Z",
     "iopub.status.busy": "2025-04-20T02:48:09.459669Z",
     "iopub.status.idle": "2025-04-20T02:48:11.462618Z",
     "shell.execute_reply": "2025-04-20T02:48:11.460480Z",
     "shell.execute_reply.started": "2025-04-20T02:48:09.460351Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- 参数设定 ---\n",
    "batch_size = 1\n",
    "seq_len = 12  # 总序列长度\n",
    "d_model = 8   # 嵌入维度 (为了清晰起见保持较小)\n",
    "num_devices = 3 # 模拟的设备/分块数量\n",
    "chunk_len = seq_len // num_devices # 每个设备上的序列块长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ebec44d-6b39-44f4-9608-a46d0a2f9903",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T02:48:18.462258Z",
     "iopub.status.busy": "2025-04-20T02:48:18.461745Z",
     "iopub.status.idle": "2025-04-20T02:48:18.470687Z",
     "shell.execute_reply": "2025-04-20T02:48:18.468235Z",
     "shell.execute_reply.started": "2025-04-20T02:48:18.462215Z"
    }
   },
   "outputs": [],
   "source": [
    "assert seq_len % num_devices == 0, \"序列长度必须能被设备数量整除\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b73e34f5-58c7-44eb-b5f7-211aaa8fe39e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T02:48:25.134967Z",
     "iopub.status.busy": "2025-04-20T02:48:25.134212Z",
     "iopub.status.idle": "2025-04-20T02:48:25.146892Z",
     "shell.execute_reply": "2025-04-20T02:48:25.144819Z",
     "shell.execute_reply.started": "2025-04-20T02:48:25.134899Z"
    }
   },
   "outputs": [],
   "source": [
    "Q = torch.randn(batch_size, seq_len, d_model)\n",
    "K = torch.randn(batch_size, seq_len, d_model)\n",
    "V = torch.randn(batch_size, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b452713c-a6c1-4bb5-8ce6-d8a3b9a7654d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T02:48:37.834565Z",
     "iopub.status.busy": "2025-04-20T02:48:37.832176Z",
     "iopub.status.idle": "2025-04-20T02:48:37.858646Z",
     "shell.execute_reply": "2025-04-20T02:48:37.857820Z",
     "shell.execute_reply.started": "2025-04-20T02:48:37.834481Z"
    }
   },
   "outputs": [],
   "source": [
    "scale = d_model ** -0.5 # 缩放因子\n",
    "# 计算注意力分数: Q @ K^T\n",
    "attn_scores_standard = torch.matmul(Q, K.transpose(-2, -1)) * scale\n",
    "# 应用 Softmax 获取注意力权重\n",
    "attn_weights_standard = F.softmax(attn_scores_standard, dim=-1)\n",
    "# 将权重应用于 V 得到输出\n",
    "output_standard = torch.matmul(attn_weights_standard, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c51da4a7-774b-41e9-a37d-64c4827a8eed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T02:48:48.967226Z",
     "iopub.status.busy": "2025-04-20T02:48:48.966518Z",
     "iopub.status.idle": "2025-04-20T02:48:48.981690Z",
     "shell.execute_reply": "2025-04-20T02:48:48.979163Z",
     "shell.execute_reply.started": "2025-04-20T02:48:48.967148Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_standard.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ab31ce-2ea7-424d-b828-9539de9e63e2",
   "metadata": {},
   "source": [
    "### ring sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9f89805-0e67-45c9-83d2-88651a53b3f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T02:49:05.865033Z",
     "iopub.status.busy": "2025-04-20T02:49:05.864367Z",
     "iopub.status.idle": "2025-04-20T02:49:05.880839Z",
     "shell.execute_reply": "2025-04-20T02:49:05.878458Z",
     "shell.execute_reply.started": "2025-04-20T02:49:05.864974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q 被切分成 3 块, 每块形状: torch.Size([1, 4, 8])\n",
      "K 被切分成 3 块, 每块形状: torch.Size([1, 4, 8])\n",
      "V 被切分成 3 块, 每块形状: torch.Size([1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "Q_chunks = list(torch.chunk(Q, num_devices, dim=1))\n",
    "K_chunks = list(torch.chunk(K, num_devices, dim=1))\n",
    "V_chunks = list(torch.chunk(V, num_devices, dim=1))\n",
    "\n",
    "print(f\"Q 被切分成 {len(Q_chunks)} 块, 每块形状: {Q_chunks[0].shape}\")\n",
    "print(f\"K 被切分成 {len(K_chunks)} 块, 每块形状: {K_chunks[0].shape}\")\n",
    "print(f\"V 被切分成 {len(V_chunks)} 块, 每块形状: {V_chunks[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed01dc84-5375-4a59-b3e2-31cc39a701fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T02:53:22.183978Z",
     "iopub.status.busy": "2025-04-20T02:53:22.183253Z",
     "iopub.status.idle": "2025-04-20T02:53:22.209964Z",
     "shell.execute_reply": "2025-04-20T02:53:22.208963Z",
     "shell.execute_reply.started": "2025-04-20T02:53:22.183915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Simulating Ring Self-Attention ---\n",
      "Split Q into 3 chunks, each shape: torch.Size([1, 4, 8])\n",
      "Split K into 3 chunks, each shape: torch.Size([1, 4, 8])\n",
      "Split V into 3 chunks, each shape: torch.Size([1, 4, 8])\n",
      "\n",
      "-- Simulating Device 0 --\n",
      "  Device 0 Q shape: torch.Size([1, 4, 8])\n",
      "  Step 0: Device 0 using K chunk from Device 0 (Shape: torch.Size([1, 4, 8]))\n",
      "    Partial scores shape for K_0: torch.Size([1, 4, 4])\n",
      "  Step 1: Device 0 using K chunk from Device 2 (Shape: torch.Size([1, 4, 8]))\n",
      "    Partial scores shape for K_2: torch.Size([1, 4, 4])\n",
      "  Step 2: Device 0 using K chunk from Device 1 (Shape: torch.Size([1, 4, 8]))\n",
      "    Partial scores shape for K_1: torch.Size([1, 4, 4])\n",
      "  Device 0: Concatenated scores shape (Correct Order): torch.Size([1, 4, 12])\n",
      "  Device 0: Softmax weights shape: torch.Size([1, 4, 12])\n",
      "  Device 0: Output chunk shape: torch.Size([1, 4, 8])\n",
      "\n",
      "-- Simulating Device 1 --\n",
      "  Device 1 Q shape: torch.Size([1, 4, 8])\n",
      "  Step 0: Device 1 using K chunk from Device 1 (Shape: torch.Size([1, 4, 8]))\n",
      "    Partial scores shape for K_1: torch.Size([1, 4, 4])\n",
      "  Step 1: Device 1 using K chunk from Device 0 (Shape: torch.Size([1, 4, 8]))\n",
      "    Partial scores shape for K_0: torch.Size([1, 4, 4])\n",
      "  Step 2: Device 1 using K chunk from Device 2 (Shape: torch.Size([1, 4, 8]))\n",
      "    Partial scores shape for K_2: torch.Size([1, 4, 4])\n",
      "  Device 1: Concatenated scores shape (Correct Order): torch.Size([1, 4, 12])\n",
      "  Device 1: Softmax weights shape: torch.Size([1, 4, 12])\n",
      "  Device 1: Output chunk shape: torch.Size([1, 4, 8])\n",
      "\n",
      "-- Simulating Device 2 --\n",
      "  Device 2 Q shape: torch.Size([1, 4, 8])\n",
      "  Step 0: Device 2 using K chunk from Device 2 (Shape: torch.Size([1, 4, 8]))\n",
      "    Partial scores shape for K_2: torch.Size([1, 4, 4])\n",
      "  Step 1: Device 2 using K chunk from Device 1 (Shape: torch.Size([1, 4, 8]))\n",
      "    Partial scores shape for K_1: torch.Size([1, 4, 4])\n",
      "  Step 2: Device 2 using K chunk from Device 0 (Shape: torch.Size([1, 4, 8]))\n",
      "    Partial scores shape for K_0: torch.Size([1, 4, 4])\n",
      "  Device 2: Concatenated scores shape (Correct Order): torch.Size([1, 4, 12])\n",
      "  Device 2: Softmax weights shape: torch.Size([1, 4, 12])\n",
      "  Device 2: Output chunk shape: torch.Size([1, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Ring Self-Attention Simulation ---\n",
    "print(\"\\n--- Simulating Ring Self-Attention ---\")\n",
    "\n",
    "# Split tensors into chunks for each \"device\"\n",
    "Q_chunks = list(torch.chunk(Q, num_devices, dim=1))\n",
    "K_chunks = list(torch.chunk(K, num_devices, dim=1))\n",
    "V_chunks = list(torch.chunk(V, num_devices, dim=1))\n",
    "\n",
    "print(f\"Split Q into {len(Q_chunks)} chunks, each shape: {Q_chunks[0].shape}\")\n",
    "print(f\"Split K into {len(K_chunks)} chunks, each shape: {K_chunks[0].shape}\")\n",
    "print(f\"Split V into {len(V_chunks)} chunks, each shape: {V_chunks[0].shape}\")\n",
    "\n",
    "output_chunks_rsa = []\n",
    "\n",
    "# Simulate computation on each device\n",
    "for i in range(num_devices):\n",
    "    print(f\"\\n-- Simulating Device {i} --\")\n",
    "    q_local = Q_chunks[i] # Query chunk for this device\n",
    "    ordered_scores = [None] * num_devices\n",
    "\n",
    "    # Ring communication for Keys\n",
    "    print(f\"  Device {i} Q shape: {q_local.shape}\")\n",
    "    for j in range(num_devices):\n",
    "        k_idx = (i - j + num_devices) % num_devices # Index of K chunk received in this step\n",
    "        k_remote = K_chunks[k_idx]\n",
    "        print(f\"  Step {j}: Device {i} using K chunk from Device {k_idx} (Shape: {k_remote.shape})\")\n",
    "\n",
    "        # Calculate partial attention scores: Q_local @ K_remote^T\n",
    "        scores_part = torch.matmul(q_local, k_remote.transpose(-2, -1)) * scale\n",
    "        print(f\"    Partial scores shape for K_{k_idx}: {scores_part.shape}\")\n",
    "        ordered_scores[k_idx] = scores_part\n",
    "\n",
    "    # Concatenate partial scores in the correct order (k=0, 1, ..., N-1)\n",
    "    all_scores_for_q_i = torch.cat(ordered_scores, dim=-1)\n",
    "    print(f\"  Device {i}: Concatenated scores shape (Correct Order): {all_scores_for_q_i.shape}\") # Should be [batch, chunk_len, seq_len]\n",
    "\n",
    "    # Apply Softmax\n",
    "    attn_weights_for_q_i = F.softmax(all_scores_for_q_i, dim=-1)\n",
    "    print(f\"  Device {i}: Softmax weights shape: {attn_weights_for_q_i.shape}\")\n",
    "\n",
    "    # Apply weights to Value matrix (using reconstructed full V for equivalence check)\n",
    "    full_V = torch.cat(V_chunks, dim=1) # Reconstruct full V for calculation\n",
    "    output_chunk_i = torch.matmul(attn_weights_for_q_i, full_V)\n",
    "    print(f\"  Device {i}: Output chunk shape: {output_chunk_i.shape}\") # Should be [batch, chunk_len, d_model]\n",
    "\n",
    "    output_chunks_rsa.append(output_chunk_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1bc3c42-af41-49ac-bdbf-96121c96e840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T02:53:30.774528Z",
     "iopub.status.busy": "2025-04-20T02:53:30.773907Z",
     "iopub.status.idle": "2025-04-20T02:53:30.789533Z",
     "shell.execute_reply": "2025-04-20T02:53:30.787361Z",
     "shell.execute_reply.started": "2025-04-20T02:53:30.774470Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RSA Result ---\n",
      "RSA Concatenated Output Shape: torch.Size([1, 12, 8])\n",
      "\n",
      "--- Comparison ---\n",
      "Are Standard Attention and Ring Attention outputs equivalent? True\n",
      "Success: The Ring Self-Attention simulation produced the same result as standard attention.\n"
     ]
    }
   ],
   "source": [
    "# Concatenate the output chunks from all devices\n",
    "output_rsa = torch.cat(output_chunks_rsa, dim=1) # Concatenate along the sequence dimension\n",
    "print(\"\\n--- RSA Result ---\")\n",
    "print(\"RSA Concatenated Output Shape:\", output_rsa.shape)\n",
    "\n",
    "# --- 3. Comparison ---\n",
    "print(\"\\n--- Comparison ---\")\n",
    "# Check if the results are numerically close\n",
    "are_close = torch.allclose(output_standard, output_rsa, atol=1e-6) # Use a tolerance\n",
    "\n",
    "print(f\"Are Standard Attention and Ring Attention outputs equivalent? {are_close}\")\n",
    "\n",
    "# Verify the shapes match\n",
    "assert output_standard.shape == output_rsa.shape, \"Shapes do not match!\"\n",
    "if are_close:\n",
    "    print(\"Success: The Ring Self-Attention simulation produced the same result as standard attention.\")\n",
    "else:\n",
    "    print(\"Failure: The results differ.\")\n",
    "    # Optional: Print difference magnitude if they differ\n",
    "    # diff = torch.abs(output_standard - output_rsa).max()\n",
    "    # print(f\"Maximum absolute difference: {diff.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casual",
   "language": "python",
   "name": "casual"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
