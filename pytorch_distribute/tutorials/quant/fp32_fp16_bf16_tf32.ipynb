{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87db10f3-0b7d-44c2-ad9c-cd6b79927dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T16:08:30.246573Z",
     "iopub.status.busy": "2024-06-11T16:08:30.245977Z",
     "iopub.status.idle": "2024-06-11T16:08:30.256247Z",
     "shell.execute_reply": "2024-06-11T16:08:30.254083Z",
     "shell.execute_reply.started": "2024-06-11T16:08:30.246526Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4151e0-f6dd-4a3b-8069-ac6dae648747",
   "metadata": {},
   "source": [
    "## dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "445570b1-75ae-45aa-bb77-861dcba59abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-11T16:09:19.227448Z",
     "iopub.status.busy": "2024-06-11T16:09:19.226787Z",
     "iopub.status.idle": "2024-06-11T16:09:19.239600Z",
     "shell.execute_reply": "2024-06-11T16:09:19.237506Z",
     "shell.execute_reply.started": "2024-06-11T16:09:19.227399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"../../imgs/tf32.jpeg\" width=\"400\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fp32\n",
    "# tf32: 没有32位\n",
    "# fp16\n",
    "# bf16\n",
    "Image(url='../../imgs/tf32.jpeg', width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a9f980-e5d7-4b28-93cb-ab8a4e1a0602",
   "metadata": {},
   "source": [
    "## AMP (automatic mixed precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e722d05-19c2-41ed-bf67-71fc06ebda95",
   "metadata": {},
   "source": [
    "```\n",
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# 创建模型和优化器\n",
    "model = ...\n",
    "optimizer = ...\n",
    "\n",
    "# 创建 GradScaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "for data, target in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 在前向传播中启用 autocast\n",
    "    with autocast():\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "\n",
    "    # 反向传播和优化步骤\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742771af-226b-4972-8cd5-159f6e755cb7",
   "metadata": {},
   "source": [
    "\n",
    "- 关键操作使用 fp32；\n",
    "    - 保留关键操作（如梯度累积和权重更新）使用 FP32，这些操作对训练稳定性和精度非常重要。\n",
    "    - 非关键操作（如前向传播中的大部分计算）使用 FP16，提高了计算效率和内存利用率。\n",
    "\n",
    "- 动态损失缩放：\n",
    "    - 动态损失缩放技术通过缩放损失值，确保在 FP16 中进行计算时不会出现溢出或下溢问题。\n",
    "    - 这有助于在训练过程中保持数值稳定，减少由于精度不足导致的训练问题。\n",
    "\n",
    "\n",
    "- 损失缩放（loss scaling）: `scaler.scale(loss)`\n",
    "    - `*scale (1024)`\n",
    "- 反向传播计算梯度：`(loss * 1024).backward()`\n",
    "- 梯度缩放还原：\n",
    "    - 在 scaler.step(optimizer) 中，会将放大的梯度还原到原始范围，即除以 1024。\n",
    "        - 具体来说，scaler.step(optimizer) 会对每个参数的梯度进行缩放还原，`param.grad = param.grad / 1024`，从而恢复梯度的实际大小。\n",
    "- 动态调整缩放因子：\n",
    "    - 在 scaler.update() 中，会检查是否在前面的步骤中发生了数值溢出。\n",
    "    - 如果检测到数值溢出，缩放因子会减小（例如乘以 0.5）。\n",
    "    - 如果没有检测到溢出，缩放因子可能会增大（例如乘以 2），以优化计算性能。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
