{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a576c25d-3333-4a29-ae40-10d21c47d7cb",
   "metadata": {},
   "source": [
    "- dual 4090：48GB\n",
    "    - pipeline parallelism，模型按layers分两班，交替执行，一段时间只有单颗核心在计算，最终性能上肯定是下降的；\n",
    "        - llama.cpp 的方案\n",
    "    - tensor parallelism：每一层模型都切成两份，每张显卡只需要存半个模型，平摊显存占用；\n",
    "      - 层内卡间存在数据交换，对通信速度要求高，但同一个时间，每一张显卡的核心和显存都占满了；\n",
    "    - pipeline parallelism < tensor parallelism "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b096fc2",
   "metadata": {},
   "source": [
    "## 基本参考"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001a2837",
   "metadata": {},
   "source": [
    "### 1B model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b384e95",
   "metadata": {},
   "source": [
    "- 10亿参数\n",
    "- float32 vs. float16\n",
    "    - float32: 4GB 内存/显存\n",
    "    - float16: 2GB 内存/显存\n",
    "- inference/train\n",
    "    - 训练至少是 2 倍于推理时候的显存资源\n",
    "        - 基本都是跟优化器（optimizer）相关\n",
    "            - gradient （momentum）\n",
    "            - optimizer state\n",
    "            - intermediate state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e470f09",
   "metadata": {},
   "source": [
    "### 训练技术"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa20aa5",
   "metadata": {},
   "source": [
    "- 3D parallel：并行的粒度和对象；\n",
    "    - tensor parallel：model layer weight matrix\n",
    "        - 模型参数矩阵，1个矩阵拆解为2个小矩阵，放在两张卡上；\n",
    "    - data parallel：\n",
    "    - layer parallel：\n",
    "        - 前面一些层放一张卡上，后边一些层也放一张卡上；\n",
    "        - 模型并行：model parallel\n",
    "- FSDP：Full Sharded Data Parallel，一种数据并行方法\n",
    "    - ZeRO-3\n",
    "- 8Bit adam optimizer / lion optimizer\n",
    "- Mixed precision\n",
    "- CPU offload\n",
    "    - 部分的训练参数和状态放在CPU的内存里；"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b983f46",
   "metadata": {},
   "source": [
    "## bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fd9886",
   "metadata": {},
   "source": [
    "- `loaded_in_8bit` 需要有 bitsandbytes 库的支撑"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbb4d03",
   "metadata": {},
   "source": [
    "```\n",
    "def is_bnb_available():\n",
    "    return importlib.util.find_spec(\"bitsandbytes\") is not None\n",
    "```\n",
    "\n",
    "```\n",
    "loaded_in_8bit = getattr(self.model, \"is_loaded_in_8bit\", False)\n",
    "if loaded_in_8bit and not is_bnb_available():\n",
    "    raise ImportError(\n",
    "        \"To use Lora with 8-bit quantization, please install the `bitsandbytes` package. \"\n",
    "        \"You can install it with `pip install bitsandbytes`.\"\n",
    "    )\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
